# ğŸ” Low-Rank Adaptation (LoRA)

## ğŸ§  Motivation

Large pre-trained language models like GPT, BERT, etc., contain billions of parameters and require massive compute and memory for fine-tuning on downstream tasks. Traditional full fine-tuning involves updating all parameters, which is inefficient, memory-intensive, and impractical for resource-constrained setups.

**LoRA (Low-Rank Adaptation)** provides a simple yet effective solution â€” instead of updating full-rank weight matrices during adaptation, it introduces a low-rank trainable update. This allows the majority of model parameters to remain frozen, significantly reducing training cost while achieving comparable performance.

---

## âš™ï¸ Core Idea

LoRA assumes that the required updates to a pre-trained modelâ€™s weights during fine-tuning lie in a low-dimensional subspace. Hence, instead of full-rank updates, LoRA adds a **low-rank decomposition** to the weight matrices.

### ğŸ”¢ Mathematical Formulation

Let `Wâ‚€ âˆˆ â„^(dÃ—k)` be a pre-trained weight matrix. LoRA modifies it as follows:


Where:
- `B âˆˆ â„^(dÃ—r)`
- `A âˆˆ â„^(rÃ—k)`
- `r â‰ª min(d, k)` (i.e., r is much smaller than d and k)

During training:
- `Wâ‚€` remains frozen (no gradients)
- Only `A` and `B` are trainable
- `Î”W = B Ã— A` is initialized such that `B = 0` and `A` is random (Î”W starts as 0)
- The final forward pass becomes:
  

Optionally, this is scaled by a factor `Î± / r` to control update magnitude.

---

## ğŸ§ª Application to Transformers

In Transformer architectures, LoRA is applied to the weight matrices inside self-attention modules:

- `Wq`, `Wk`, `Wv`, `Wo`

In most setups, LoRA is used only on the `Wq` and `Wv` matrices for simplicity and efficiency. The MLP blocks and biases remain frozen.

---

## âœ… Benefits of LoRA

- ğŸ”‹ **Memory Efficient**: Reduces GPU VRAM usage drastically. (e.g., GPT-3 175B fine-tuning VRAM drops from 1.2TB to 350GB)
- ğŸ’¾ **Smaller Checkpoints**: Checkpoint size reduced by 10,000Ã— (e.g., from 350GB to 35MB with r = 4)
- ğŸš€ **Faster Training**: 25% speedup in training due to fewer parameters being updated
- ğŸ”„ **Flexible Deployment**: Easily switch between tasks by swapping LoRA weights (A, B), while the base model remains constant
- âš¡ **No Additional Inference Latency**: Can merge `Wâ‚€ + Î”W` after training for efficient inference

---

## âš ï¸ Limitations

- âŒ Not ideal for batching multiple tasks with different LoRA modules in the same forward pass
- ğŸ”„ If weights are merged (for zero-latency inference), task switching requires explicit unmerging and re-merging

---

## ğŸ“š Conclusion

LoRA offers a highly scalable and efficient way to fine-tune large models with minimal compute and memory. It enables multi-task adaptation, rapid deployment, and reduced storage requirements â€” making it a practical choice for fine-tuning massive language models.

